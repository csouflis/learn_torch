{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divine-spell",
   "metadata": {},
   "source": [
    "### Learning Pytorch\n",
    "### Pytorch led by Facebook AI Research (FAIR) is a minimal library built off Torch as a python extension of Lua which can directly use CUDA GPUs for accelerated processing - and has good codeflow.\n",
    "\n",
    "\n",
    "### The main modules of pytorch as as below and they are self explanatory in sense. \n",
    "\n",
    "<img src = \"images/pytorch_modules.png\">\n",
    "\n",
    "<br><br>\n",
    "- torch.nn = Base class for all neural network modules.\n",
    "    - torch.nn.functional = functionality to blocks of network\n",
    "<br><br>\n",
    "- torch.Tensor = multi-dimensional matrix containing elements of a single data type.\n",
    "<br><br>\n",
    "- torch.autograd = provides classes and functions implementing automatic differentiation. A recorder records what operations have performed, and then it replays it backward to compute the gradients. This method is especially powerful when building neural networks to save time on one epoch by calculating differentiation of the parameters at the forward pass. \n",
    "<br><br>\n",
    "- torch.utils.data = the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-thermal",
   "metadata": {},
   "source": [
    "#### Make sure you have an virutal env set up with anaconda, and conda installed pytorch, cudatoolkit, and jupyter.\n",
    "\n",
    "### Verify the install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should succeed without hesitation\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify you have CUDA-supported NVIDIA GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should match the version of cudatoolkit installed\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-minneapolis",
   "metadata": {},
   "source": [
    "### Sidenote: Why GPUs\n",
    "For tasks such as mathematical computation, it is possbile to achieve solution through parallel computing because even complex equations can be broken down into smaller computations that are independent with respect to one another. GPUs are much more capable of parallel computation because of their massive core counts.\n",
    "\n",
    "CUDA is a API library that has to be downloaded (like cudatoolkit) to enable this feature on NVIDIA GPUs.\n",
    "\n",
    "Pytorch is verstile in that it can selectively hold mathematical objects such as tensors on different devices such as the GPU or CPU. \n",
    "#### Why not use a GPU for everything?\n",
    "Moving simple computation into GPU will slow down code even more because the time it would take a modern CPU to complete the code can be done by the time the GPU receives the task. Remember GPUs are only amazing densely mathematical tasks that can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply creating Pytorch tensor object\n",
    "t = torch.Tensor([1,2,3])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing GPU\n",
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensor object to GPU\n",
    "t = t.cuda()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-gabriel",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "Tensors, in short, is a general term to describe a representation of numbers which can have rank, axes, and shapes. Think about an matrix, but with more dimensionability. \n",
    "\n",
    "Number of dimensions present within a tensor. How many indices you need to access an element - for example the above tensor 't' has a rank of 1 because each element only needs one index for access like t[1]. Axes are measures of which elements can exist, and multiple axes create more dimensions for elements to exist. \n",
    "\n",
    "Shapes of tensors encode all relevant info about rank, axes and even indices. Tensors are constantly reshaped and transformed (which is why their named tensors). Tensors can represent the same underlying data but have different shapes. Think about it: if 'a' was a list of lengths of toothpicks (or something weird like that) in the code below, changing its shape wouldn't change those lengths of the the toothpicks.\n",
    "\n",
    "\n",
    "Shapes can be more than 2 or 3 dimensions. For example, an image can typically have three dimensions represented by height, width, and RGB value of a pixel: [C,H,W]. However, in deep learning, more dimensions can be added, such as batch size (an amount of images in a subgroup). Now an image will have 4 dimensions and look like: [B,C,H,W]. So now as a rank 4 tensor we can navigate to a specific pixel in a specific image with a specific color. \n",
    "\n",
    "Features can be extracted from these values by transforming them with specific operations. See more later on this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to tensors; different tensor calls create different tensor types but still same shape\n",
    "b = torch.tensor(a)\n",
    "c = torch.Tensor(a)\n",
    "print(type(b), b.dtype, b)\n",
    "print(type(c), c.dtype,  c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping a tensor\n",
    "b.reshape(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.reshape(1,6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember that calling reshape doesn't actually change b as a variable, only for runtime not storage\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1,2,3])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-surge",
   "metadata": {},
   "source": [
    "#### Different ways to initiate a tensor and exploring their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice this is a different construction of a tensor then the next 4: Pytorch Class Constructor of a tensor  - unchanged by numpy array manipulation\n",
    "\n",
    "t1 = torch.Tensor(data)\n",
    "print(t1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#factory function (accepts parameter inputs and outputs tensor objects) - unchanged by numpy array manipulation\n",
    "\n",
    "t2 = torch.tensor(data)\n",
    "print(t2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also a factory function --> changing the numpy array (variable 'data') will change these values\n",
    "\n",
    "t3 = torch.as_tensor(data)\n",
    "print(t3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also a factory function --> changing the numpy array (variable 'data') will change these values\n",
    "\n",
    "t4 = torch.from_numpy(data)\n",
    "print(t4.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-rebound",
   "metadata": {},
   "source": [
    "### Recap\n",
    "torch.Tensor(data) & torch.tensor(data) = **data is copied** but also different data types\n",
    "<br>\n",
    "torch.as_tensor(data) & torch.from_numpy(data) = **data is shared**(saves memory space) and same data types\n",
    "<br><br>\n",
    "torch.tensor() = best for everyday use\n",
    "<br>\n",
    "torch.as_tensor() = best for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type also is inferred by incoming data\n",
    "print(torch.get_default_dtype()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type inference - what you put in is what it becomes\n",
    "a = torch.tensor(np.array([1,2,3]))\n",
    "print(a)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.array([1.,2.,3.]))\n",
    "print(a)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly set\n",
    "a = torch.tensor(np.array([1,2,3]), dtype=torch.float64)\n",
    "print(a)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-jurisdiction",
   "metadata": {},
   "source": [
    "#### Creating multi-dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.zeros(3,2))\n",
    "print(torch.zeros(3,2).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.ones(2,3))\n",
    "print(torch.ones(2,3).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#between 0-1 and four decimals out\n",
    "torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-stadium",
   "metadata": {},
   "source": [
    "# Main Tensor operation types:\n",
    "\n",
    "   **1. Reshaping operations**\n",
    "   <br><br>2. Element-wise operations\n",
    "   <br>3. Reduction operations\n",
    "   <br>4. Access operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-airfare",
   "metadata": {},
   "source": [
    "##    1. Reshaping operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank 2 tensor with two axes; one axis with length of 4 and made up of arrays,\n",
    "#   the other axis has length of 3 and is made up of numbers\n",
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of shape gives rank of a tensor\n",
    "len(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-cassette",
   "metadata": {},
   "source": [
    "Find the number of elements in tensor product of dimensions or numel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(t.shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-possible",
   "metadata": {},
   "source": [
    "All reshaping operations for this 3 by 4 tensor will always have a product of 12 b/c the elements (and # of elements) is unchanged\n",
    "\n",
    "Sidenote: that's kinda why their called tensors. Tensor elements are unchanged when ungoing transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reshape(1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reshape(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reshape(6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reshape(12,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change the rank!\n",
    "t.reshape(2,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-given",
   "metadata": {},
   "source": [
    "**Squeezing** allows to one to change the rank of tensor without even changing the shape (doesn't change rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.reshape(1,12))\n",
    "print(t.reshape(1,12).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.reshape(1,12).squeeze())\n",
    "print(t.reshape(1,12).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0))\n",
    "print(t.reshape(1,12).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-inspector",
   "metadata": {},
   "source": [
    "**Flattening** removes all axes except one, making it a rank 1 tensor (change rank to 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1,-1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape and flatten (removes a set of brackets!)!\n",
    "flatten(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape only\n",
    "t.reshape(1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing as above\n",
    "t.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-symphony",
   "metadata": {},
   "source": [
    "**Concatenating** tensors (doesn't change rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "t2 = torch.tensor([\n",
    "    [5,6],\n",
    "    [7,8]\n",
    "])\n",
    "\n",
    "#original shape\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine t1 and t2 row-wise (axis-0)\n",
    "torch.cat((t1, t2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them column-wise (axis-1) \n",
    "torch.cat((t1, t2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis 0 because listed first --> adding on axis 0 adds 2 to first number\n",
    "torch.cat((t1, t2), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#likewise for axis 1\n",
    "torch.cat((t1, t2), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-trauma",
   "metadata": {},
   "source": [
    "### CNN Flatten Operations - Tensor Batch Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three rank 2 tensors with shape (4,4)\n",
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack is a concat function\n",
    "t = torch.stack((t1,t2,t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank 3 tensor\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we've seen we change increase the rank of a tensor with the reshape operation\n",
    "t = t.reshape(3,1,4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-flavor",
   "metadata": {},
   "source": [
    "That tensor has lots of information stored. For example if each original tensor (t1, t2, t3) were 4 by 4 pixel grayscale images, then this final tensor *t* would have information on (batch, channel, height, width)\n",
    "\n",
    "Notice how the additional axis of length 1 as a channel doesn't change the number of elements in the tensor. This is because the product of the components values doesn't change when we multiply by one. \n",
    "\n",
    "#### The first axis has 3 elements. Each element of the first axis represents an image (an image in a batch). For each image, we have a single color channel on the channel axis (b/c it's grayscale). Each of these channels contain 4 arrays that contain 4 numbers or scalar components.\n",
    "\n",
    "Observe this by indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first image\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first color channel in first image\n",
    "t[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the first color channel of the first image\n",
    "t[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pixel value in the first row of the first color channel of the first image\n",
    "t[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattens entire batch\n",
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave batch tensor intact to maintain images seperate\n",
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten by the batch index (1)\n",
    "t.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-pregnancy",
   "metadata": {},
   "source": [
    "   1. Reshaping operations\n",
    "  \n",
    "   **2. Element-wise operations**\n",
    "   <br><br>3. Reduction operations\n",
    "   <br>4. Access operations\n",
    "\n",
    "## Element-wise Tensor Operation\n",
    "\n",
    "An element-wise operation operates on corresponding elements between tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [9,8],\n",
    "    [7,6]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the first axis to call\n",
    "print('first axis: ',t1[0])\n",
    "\n",
    "# Example of the second axis to call\n",
    "print('second axis: ',t1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1[0][0])\n",
    "print(t2[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-martin",
   "metadata": {},
   "source": [
    "These are elements of a tensors. They can be called upon by their positions within the axes or dimensions within the tensor.\n",
    "\n",
    "The correspondence of this tensor is defined by the indexes.\n",
    "\n",
    "### All Arithmetic operations (add, subtract, multiply, divide) are Element-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-option",
   "metadata": {},
   "outputs": [],
   "source": [
    " t1 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 / t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 * t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are also built-in tensor object methods for arithmetic operations\n",
    "print(t1.add(2),'\\n')\n",
    "print(t1.add(2),'\\n')\n",
    "print(t1.mul(2),'\\n')\n",
    "print(t1.div(2),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-alert",
   "metadata": {},
   "source": [
    "These examples are breaking the rule we established that said element-wise operations operate on tensors of the same shape. Scalar values are Rank-0 tensors, which means they have no shape, and our tensor t1 is a rank-2 tensor of shape 2 x 2.\n",
    "\n",
    "So how does this fit in? Let's break it down.\n",
    "\n",
    "The first solution that may come to mind is that the operation is simply using the single scalar value and operating on each element within the tensor. This logic kind of works. However, it's a bit misleading, and it breaks down in more general situations where we're note using a scalar.\n",
    "\n",
    "To think about these operations differently, we need to introduce the concept of tensor broadcasting or broadcasting.\n",
    "### Broadcasting tensors \n",
    " Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.broadcast_to(2, t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting is under the hood\n",
    "t1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 + torch.tensor(\n",
    "    np.broadcast_to(2, t1.shape),\n",
    "    dtype=torch.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-portrait",
   "metadata": {},
   "source": [
    "Broadcasting is basically making the shapes the same for compatibility of the operation. It can even work for other tensors, not just scalars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank 1 shape=[2, 2]\n",
    "t1 = torch.ones([2,2], dtype=torch.float32)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank2 shape=[2]\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing\n",
    "np.broadcast_to(t2.numpy(),[2,2]) \n",
    "np.broadcast_to(t2.numpy(),t1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor broadcasting to perform operations\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-contemporary",
   "metadata": {},
   "source": [
    "### Comparison operations are also element-wise operations.\n",
    "\n",
    "For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing either a torch.bool value of *True or False*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,5,0],\n",
    "    [6,0,7],\n",
    "    [0,8,0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "spare-baptist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ge(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "wired-framing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "alleged-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.le(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "judicial-report",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "surgical-ribbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# under the hood**\n",
    "t <= torch.tensor(\n",
    "    np.broadcast_to(7, t.shape)\n",
    "    ,dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "expanded-vitamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing (equivilant tensor under the hood\n",
    "t <= torch.tensor([\n",
    "    [7,7,7],\n",
    "    [7,7,7],\n",
    "    [7,7,7]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-plate",
   "metadata": {},
   "source": [
    "With element-wise operations that are functions, it's fine to assume that the function is applied to each element of the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "sunrise-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 5., 0.],\n",
       "        [6., 0., 7.],\n",
       "        [0., 8., 0.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "silver-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.2361, 0.0000],\n",
       "        [2.4495, 0.0000, 2.6458],\n",
       "        [0.0000, 2.8284, 0.0000]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "basic-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -5., -0.],\n",
       "        [-6., -0., -7.],\n",
       "        [-0., -8., -0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-brass",
   "metadata": {},
   "source": [
    "   1. Reshaping operations\n",
    "   2. Element-wise operations\n",
    "   **3. Reduction operations**\n",
    "   <br>\n",
    "   <br>\n",
    "   4. Access operations\n",
    "\n",
    "## Reduction Operations\n",
    "\n",
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "<br>\n",
    "Tensors give us the ability to manage our data.\n",
    "<br>\n",
    "<br>\n",
    "Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow us to perform operations on elements between two tensors, and **reduction operations allow us to perform operations on elements within a single tensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "rolled-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,1,0],\n",
    "    [2,0,2],\n",
    "    [0,3,0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "tamil-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "developed-martial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get NUMber of ELements in a tensor\n",
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "adult-dining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum().numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "celtic-answer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum().numel() < t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-burden",
   "metadata": {},
   "source": [
    "Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a reduction operation.\n",
    "\n",
    "**Other Common tensor reduction operations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "human-puppy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "rough-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "engaged-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8889)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "nervous-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1667)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-binary",
   "metadata": {},
   "source": [
    "*Friendly Reminder<br>\n",
    "To calculate the standard deviation of those numbers:*\n",
    "\n",
    "    1. Work out the Mean (the simple average of the numbers)\n",
    "    2. Then for each number: subtract the Mean and square the result\n",
    "    3. Then work out the mean of those squared differences.\n",
    "    4. Take the square root of that and we are done!\n",
    "But...<br>\n",
    "  Do reduction operations always reduce to a tensor with a single element? \n",
    "  <br>NO<br><br>\n",
    "#### We can reduce specific axes at a time.<br>\n",
    "It's just like we saw with reshaping when we aimed to flatten the image tensors within a batch while still maintaining the batch axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bound-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 x 4 rank-2 tensor\n",
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "atomic-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "prescribed-internship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-organization",
   "metadata": {},
   "source": [
    "Confused?? Let's go over what happened.\n",
    "\n",
    "### Understanding reductions by axes\n",
    "\n",
    "We'll tackle the first axis first. When take the summation of the first axis, we are summing the elements of the first axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "coordinated-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "better-soccer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "continuing-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "general-nature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0] + t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "advised-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0] + t[1] + t[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-astrology",
   "metadata": {},
   "source": [
    "### Reduction by axes happens through element-wise operations, and specially by broadcasting tensors upon each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "municipal-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "superb-boards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "united-algebra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "banner-vacation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-experience",
   "metadata": {},
   "source": [
    "### Argmax tensor reduction operation \n",
    "**Argmax returns the index location of the maximum value inside a tensor.**\n",
    "\n",
    "When we call the <code>argmax()</code> method on a tensor, the tensor is reduced to a new tensor that contains an index value indicating where the max value is inside the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "solar-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,0,0,2],\n",
    "    [0,3,3,0],\n",
    "    [4,0,0,5]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "accessible-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "short-nature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "lasting-saver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-warner",
   "metadata": {},
   "source": [
    "#### If we don't specific an axis to the <code>argmax()</code> method, it returns the index location of the max value from the flattened tensor, which in this case is indeed 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "expected-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4., 3., 3., 5.]),\n",
       "indices=tensor([2, 1, 1, 2]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "applied-conversation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 2])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "funny-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([2., 3., 5.]),\n",
       "indices=tensor([3, 1, 3]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "animated-employment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 3])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-oracle",
   "metadata": {},
   "source": [
    "Notice how the call to the **max() method returns two tensors**. The first tensor contains the max values and the second tensor contains the index locations for the max values. This is what argmax gives us. <br><br>\n",
    "*First Tensor*: For the first axis, the max values are, 4, 3, 3, and 5. These values are determined by taking the **element-wise maximum across each array running across the first axis.** <br><br>\n",
    "*Second Tensor*: For the second axis, the max values are 2, 3, and 5. These values are determined by taking the maximum inside each array of the first axis. We have three groups of four, which gives us 3 maximum values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-reading",
   "metadata": {},
   "source": [
    "<br>The argmax values here, tell the index inside each respective array where the max value lives.\n",
    "\n",
    "### In practice, we often use the argmax() function on a network's output prediction tensor, to determine which category has the highest prediction value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-canyon",
   "metadata": {},
   "source": [
    "   1. Reshaping operations\n",
    "   2. Element-wise operations\n",
    "   3. Reduction operations\n",
    "   <br>\n",
    "   <br>\n",
    "   **4. Access operations**\n",
    "## Accessing Elements Inside Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fifty-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "caroline-lawrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "biblical-lambda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "secret-interstate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "revised-creek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-professional",
   "metadata": {},
   "source": [
    "We can **access** the numeric values by transforming the output tensor into a Python list or a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-seventh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
